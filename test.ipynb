{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890a252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory added to PATH: /Users/daehyeon/Documents/Project/Paligemma\n",
      "Module cache cleared for forced reload.\n",
      "All custom modules loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "print(f\"Project directory added to PATH: {current_dir}\")\n",
    "\n",
    "# 캐시초기화\n",
    "if 'processing_paligemma' in sys.modules:\n",
    "    del sys.modules['processing_paligemma']\n",
    "if 'modeling_gemma' in sys.modules:\n",
    "    del sys.modules['modeling_gemma']\n",
    "if 'utils' in sys.modules:\n",
    "    del sys.modules['utils']\n",
    "print(\"Module cache cleared for forced reload.\")\n",
    "\n",
    "\n",
    "from processing_paligemma import PaliGemmaProcessor\n",
    "from modeling_gemma import KVCache, PaliGemmaForConditionalGeneration\n",
    "from utils import load_hf_model_mps_optimized \n",
    "\n",
    "print(\"All custom modules loaded successfully.\")\n",
    "\n",
    "MODEL_PATH = \"paligemma-weights/paligemma-3b-pt-224\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74601ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"paligemma-weights/paligemma-3b-pt-224\" \n",
    "PROMPT = \"What objects are present? Where are they located? What are they doing? What is their appearance? What is the setting or background?\"\n",
    "IMAGE_FILE_PATH = \"test_images/sample.jpg\" #이미지 경로\n",
    "MAX_TOKENS_TO_GENERATE = 100\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.9\n",
    "DO_SAMPLE = True\n",
    "ONLY_CPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c2021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if not ONLY_CPU:\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "print(f\"Device set to: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d56784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading model from paligemma-weights/paligemma-3b-pt-224\n",
      "Target device: mps\n",
      "Creating model instance and loading state dict manually...\n",
      "Model state dict loaded successfully.\n",
      "Error loading tokenizer or setting device: MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 384.00 KiB, max allowed: 9.07 GiB). Tried to allocate 8.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 384.00 KiB, max allowed: 9.07 GiB). Tried to allocate 8.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# load_hf_model_mps_optimized 함수를 사용하면 Mac 환경에 맞게 로드됩니다.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model, tokenizer = \u001b[43mload_hf_model_mps_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Paligemma/utils.py:68\u001b[39m, in \u001b[36mload_hf_model_mps_optimized\u001b[39m\u001b[34m(model_path, device)\u001b[39m\n\u001b[32m     64\u001b[39m try:\n\u001b[32m     65\u001b[39m     tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n\u001b[32m     67\u001b[39m     # 모델을 최종 디바이스로 이동 및 eval 모드 설정\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     # utils.py 파일의 해당 부분을 찾아서 아래처럼 수정하세요.\n\u001b[32m     69\u001b[39m \n\u001b[32m     70\u001b[39m # 4. 토크나이저 로드 및 마무리\n\u001b[32m     71\u001b[39m     try:\n\u001b[32m     72\u001b[39m         tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Paligemma/paligemma_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Paligemma/paligemma_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Paligemma/paligemma_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Paligemma/paligemma_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Paligemma/paligemma_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/Paligemma/paligemma_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 384.00 KiB, max allowed: 9.07 GiB). Tried to allocate 8.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# 모델 로딩\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_hf_model_mps_optimized(MODEL_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba535bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로세서 초기화\n",
    "num_image_tokens = model.config.vision_config.num_image_tokens\n",
    "image_size = model.config.vision_config.image_size\n",
    "processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(\"Model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#추론 실행 함수\n",
    "\n",
    "def get_model_inputs(\n",
    "    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n",
    "):\n",
    "    \"\"\"이미지 및 프롬프트 전처리 및 디바이스 이동\"\"\"\n",
    "    image = Image.open(image_file_path).convert(\"RGB\")\n",
    "    images = [image]\n",
    "    prompts = [prompt]\n",
    "    model_inputs = processor(text=prompts, images=images)\n",
    "    \n",
    "    # 디바이스 이동\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "def run_inference():\n",
    "    \"\"\"메인 추론 실행\"\"\"\n",
    "    \n",
    "    model_inputs = get_model_inputs(processor, PROMPT, IMAGE_FILE_PATH, device)\n",
    "    input_ids = model_inputs[\"input_ids\"]\n",
    "    attention_mask = model_inputs[\"attention_mask\"]\n",
    "    pixel_values = model_inputs[\"pixel_values\"]\n",
    "    kv_cache = KVCache()\n",
    "\n",
    "    stop_token = processor.tokenizer.eos_token_id\n",
    "    generated_tokens = []\n",
    "\n",
    "    print(\"\\n--- Running Inference ---\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_TOKENS_TO_GENERATE):\n",
    "            # 모델 순전파\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                kv_cache=kv_cache,\n",
    "            )\n",
    "            kv_cache = outputs[\"kv_cache\"]\n",
    "            next_token_logits = outputs[\"logits\"][:, -1, :]\n",
    "\n",
    "            # 샘플링/결정\n",
    "            if DO_SAMPLE:\n",
    "                next_token_logits = torch.softmax(next_token_logits / TEMPERATURE, dim=-1)\n",
    "                next_token = _sample_top_p(next_token_logits, TOP_P) # _sample_top_p 정의 필요\n",
    "            else:\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                \n",
    "            next_token = next_token.squeeze(0)\n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            if next_token.item() == stop_token:\n",
    "                break\n",
    "\n",
    "            # 다음 턴 입력 업데이트\n",
    "            input_ids = next_token.unsqueeze(-1)\n",
    "            attention_mask = torch.cat(\n",
    "                [attention_mask, torch.ones((1, 1), device=input_ids.device)], dim=-1\n",
    "            )\n",
    "            \n",
    "            # 진행 상황 출력 (옵션)\n",
    "            sys.stdout.write(tokenizer.decode(next_token.cpu().numpy()))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        generated_tokens = torch.cat(generated_tokens, dim=-1)\n",
    "        decoded = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        print(\"\\n\\n--- Final Result ---\")\n",
    "        print(PROMPT + decoded)\n",
    "\n",
    "\n",
    "# run_inference() # 실행"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paligemma_venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
